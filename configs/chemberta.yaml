name: "chemberta_finetune"

model:
  type: "ChemBERTa"
  pretrained_model: "seyonec/ChemBERTa-zinc-base-v1"
  freeze_encoder: false  # Fine-tune all layers
  dropout: 0.1

data:
  featurizer: "smiles_text"

training:
  lr: 0.00002  # 2e-5 (typical for BERT fine-tuning)
  batch_size: 16
  max_epochs: 10
  patience: 5
  weight_decay: 0.01
  seed: 42